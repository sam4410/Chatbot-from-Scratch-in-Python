{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMqsqRaZ84nYWgaybkdqKtN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sam4410/Chatbot-from-Scratch-in-Python/blob/main/Chatbot_from_Scratch_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Fsq4Y_zkkAUR"
      },
      "outputs": [],
      "source": [
        "# Mounting the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('content/drive')\n",
        "data_root = 'content/drive/My Drive/Chatbot'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import json\n",
        "import string\n",
        "import random\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "id": "Yhdj0q-woXBF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7be8b7f-90cf-402b-de39-bc483470b4cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading dataset\n",
        "data_file = open(data_root + '/intents.json').read()\n",
        "data = json.loads(data_file)"
      ],
      "metadata": {
        "id": "Tqr_zhczo9Nc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating data_X and data_Y\n",
        "words = []   # for BoW model/Vocab for patterns\n",
        "classes = [] # for BoW model/Vocab for tags\n",
        "data_X = [] # for storing each pattern\n",
        "data_Y = [] # for storing each tag corresponding to each pattern in data_X\n",
        "\n",
        "# Iterating over all the intents\n",
        "for intent in data['intent']:\n",
        "  for pattern in intent['patterns']:\n",
        "    tokens = nltk.word_tokenize(pattern)    # tokenize each pattern\n",
        "    words.extend(tokens)                    # append pattern tokens to words list\n",
        "    data_X.append(pattern)                  # appending patterns to data_X\n",
        "    data_Y.append(intent['tag'])            # appending the corresponding tag to data_Y\n",
        "\n",
        "    # adding the tag to classes\n",
        "    if intent[\"tag\"] no in classes:\n",
        "      classes.append(intent[\"tag\"])\n",
        "\n",
        "# initialize lemmatizer to get stem of words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# lemmatize all the words in vocab and convert them to lowercase\n",
        "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
        "\n",
        "# sorting the vocab and classes in alphabetical order\n",
        "words = sorted(set(words))\n",
        "classes = sorted(set(classes))"
      ],
      "metadata": {
        "id": "Bj5mA8bO_20q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making the data machine friendly by converting text into numbers using BoW model\n",
        "training = []\n",
        "out_empty = [0]*len(classes)\n",
        "\n",
        "# creating the bag of words model\n",
        "for idx, doc in enumerate(data_X):\n",
        "  bow = []\n",
        "  text = lemmatizer.lemmatize(doc.lower())\n",
        "  for word in words:\n",
        "    bow.append(1) if word in text else bow.append(0)\n",
        "\n",
        "  # mark the index of class that the current pattern is associated to\n",
        "  output_row = list(out_empty)\n",
        "  output_row[classes.index(data_Y[idx])] = 1\n",
        "\n",
        "  # add the 1 hot encoded BoW and associated classes to training\n",
        "  training.append([bow, output_row])\n",
        "\n",
        "# shuffle the data and convert it into array\n",
        "random.shuffle(training)\n",
        "training = np.array(training, dtype=object)\n",
        "\n",
        "# split the features and target labels\n",
        "train_X = np.array(list(training[:, 0]))\n",
        "train_Y = np.array(list(training[:, 1]))"
      ],
      "metadata": {
        "id": "7GRd6rwkCICe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape = (len(train_X[0]),), activation=\"relu\"))\n",
        "model.add(Dropout=0.5)\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dropout=0.5)\n",
        "model.add(Dense(len(train_Y[0]), activation=\"softmax\"))\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.01, decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary)\n",
        "model.fit(x=train_X, y=train_Y, epochs=150, verbose=1)"
      ],
      "metadata": {
        "id": "kZwNWXqTEJxy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing the input - User's query\n",
        "def clean_text(text):\n",
        "  tokens = nltk.word.tokenize(text)\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "  return tokens\n",
        "\n",
        "def bag_of_words(text, vocab):\n",
        "  tokens = clean_text(text)\n",
        "  bow = [0] * len(vocab)\n",
        "  for w in tokens:\n",
        "    for idx, word in enumerate(vocab):\n",
        "      if word == w:\n",
        "        bow[idx] = 1\n",
        "  return np.array(bow)\n",
        "\n",
        "def pred_class(text, vocab, labels):\n",
        "  bow = bag_of_words(text, vocab)\n",
        "  result = model.predict(np.array([bow]))[0]   # extracting probabilities\n",
        "  thresh = 0.5\n",
        "  y_pred = [[indx, res] for indx, res in enumerate(result) if res > thresh]\n",
        "  y_pred.sort(key=lambda x: x[1], reverse = True)\n",
        "  return_list = []\n",
        "\n",
        "  for r in y_pred:\n",
        "    return_list.append(labels[r[0]])    # contains labels (tags) for highest probability\n",
        "  return return_list\n",
        "\n",
        "def get_response(intents_list, intents_json):\n",
        "  if len(intents_list) == 0:\n",
        "    result = \"Sorry! I don't understand\"\n",
        "  else:\n",
        "    tag = intents_list[0]\n",
        "    list_of_intents = intents_json[\"intents\"]\n",
        "    for i in list_of_intents:\n",
        "      if i[\"tag\"] == tag:\n",
        "        result = random.choice(i[\"responses\"])\n",
        "        break\n",
        "    return result"
      ],
      "metadata": {
        "id": "kH8TXguRGAHV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean_text(text): This function receives text (string) as an input and then tokenizes it using the nltk.word_tokenize(). Each token is then converted into its root form using a lemmatizer. The output is basically a list of words in their root form.\n",
        "\n",
        "Bag_of_words(text, vocab): This function calls the above function, converts the text into an array using the bag-of-words model using the input vocabulary, and then returns the same array.\n",
        "\n",
        "Pred_class(text, vocab, labels): This function takes text, vocab, and labels as input and returns a list that contains a tag corresponding to the highest probability.\n",
        "\n",
        "Get_response(intents_list, intents_json): This function takes in the tag returned by the previous function and uses it to randomly choose a response corresponding to the same tag in intent.json. And, if the intents_list is empty, that is when the probability does not cross the threshold, we will pass the string “Sorry! I don’t understand” as ChatBot’s response."
      ],
      "metadata": {
        "id": "XzL_NkcwNtwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the relevant functions to interact with chatbot\n",
        "print(\"Press 0 if you don't want to chat with our Chatbot\")\n",
        "while True:\n",
        "  message = input(\"\")\n",
        "  if message == \"0\":\n",
        "    break\n",
        "  intents = pred_class(message, words, classes)\n",
        "  result = get_response(intents, data)\n",
        "  print(result)"
      ],
      "metadata": {
        "id": "nAgAjeP7MiJl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xw8w9BnBQMuo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}